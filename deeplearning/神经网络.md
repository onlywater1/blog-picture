# 神经网络

## 综述

神经网络的应用

![image-20250424074947089](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931391.png)

## 二分类问题理解神经网络

x是输入数据 如果是64*64的三色图片，则输入的维度是12288 y是二分类{0，1}

![image-20250424081709750](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931392.png)

输入特征通常使用列排序，

![image-20250424081824305](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931393.png)

通常把w，b分开定义，不使用红色的表示

![image-20250424082517845](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931394.png)

### 逻辑回归的损失函数

![image-20250424083951151](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931396.png)

逻辑回归损失函数的由来  log不改变增长的趋势

对于一个例子

![image-20250425092023239](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931397.png)

对于有n个训练集

![image-20250425092134884](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931398.png)

梯度下降

![image-20250424085231400](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931399.png)

### 计算图

![image-20250424091831176](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931400.png)

根据计算图求导--反向传播

![image-20250424181952928](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931401.png)

用计算图来求逻辑回归

![image-20250424183434333](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931402.png)

逻辑回归实现一次梯度下降  for循环遍历数据，求全部的损失函数，然后最后再除以m，对于参数也是同样的方法

![image-20250424184536166](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931403.png)

### 向量化逻辑回归---正向传播

![image-20250424220050499](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931404.png)

反向传播

![image-20250425084642659](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931405.png)

全部向量化

![image-20250425084709691](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931406.png)

广播的小技巧

不使用秩为1的数组，比如第一个，创建的时候建议创建成一个列向量/行向量，后面两个，如果想判断矩阵是不是自己想要的，那么就使用assert，或者把矩阵变成自己想要的reshape

![image-20250425090543344](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931407.png)

## 神经网络

表示方法

![image-20250425094900360](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931408.png)

### 如何计算的

向量化的技巧就是把参数竖着堆叠起来 --可以同时计算一整层-----竖向堆叠

![image-20250425100015874](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931409.png)

![image-20250425100110114](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931410.png)

同时计算所有的训练集--横向堆叠

![image-20250425163900227](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931411.png)

具体细节

![image-20250425165040095](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931412.png)

### 激活函数

![image-20250425170243283](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931413.png)

为什么需要激活函数

不适用激活函数，一个大的神经网络最终就是一个线性回归

![image-20250425170856207](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931415.png)

不同激活函数的导数

sigmoid

![image-20250425205427426](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931416.png)

tanh

![image-20250425205449514](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931417.png)

relu

![image-20250425205509449](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931418.png)

神经网络的梯度下降

![image-20250425210931730](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931419.png)

![image-20250425210952130](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931420.png)

具体的一些细节

注意dz1的维度，反向传播的六个主要公式（红色）

![image-20250425214047627](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931421.png)

总结

![image-20250425214135433](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931422.png)

初始化参数

注意事项：不可以将w初始化为0，因为a11，a12都是一样的，这样dz1和dz2也是一样的，导致dw是每一行都一样，从而w每一行是一样的，

![image-20250425215415888](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931423.png)

随机初始化

要使用随机初始化来初始化参数，通常w后面要乘0.01（很小的数）是为了梯度下降的时候好下，如何w的数值比较到，那么当使用tanh或者sigmoid作为激活函数的时候，梯度趋于0了训练的很慢。

注意b可以初始化为0

![image-20250425215547623](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931424.png)

### 深度神经网络

![image-20250426101810504](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931425.png)

前向传播

注意要缓存z，因为反向传播的时候会用到z

![image-20250426102619836](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931426.png)

![image-20250426105350486](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931427.png)

反向传播

![image-20250426102641304](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931428.png)

总结

![image-20250426104118126](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931429.png)

注意缓存z，w，b

![image-20250426172706473](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931430.png)

#### 核对矩阵的维度

![image-20250426111452789](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931431.png)

![image-20250426111532734](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931432.png)

#### 知识点

参数和超参数

参数就是w，b，超参数就是控制w，b的参数，比如学习率，迭代次数...

![image-20250426173524619](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931433.png)

偏差/方差

出现几个情况，高方差、高偏差、正好、即高方差又高偏差 --都是对于人类判断为标准的 

高方差--过拟合，训练集表现较好，测试集表现不好

高偏差--欠拟合，训练集表现不好



![image-20250427172357973](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931434.png)

高偏差/高方差的解决方式

高偏差--扩大神经网络、训练更长的时间、换一个神经网络结构

高方差--增大训练量、正则化、换一个神经网络结构

![image-20250427173313628](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931435.png)

#### 优化算法

##### 正则化

从逻辑回归开始认识正则化，注意是l2正则化还是L1正则化

![image-20250427174558475](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931436.png)

神经网络中的正则化

添加的是所以w参数的平方的和再乘lambda/2m  m是训练集的长度   叫做弗罗贝尼乌斯范数

lambda越大 w参数就会越小，对于模型的影响就会越小

![image-20250427174642166](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931437.png)

dropout

使用inverted dropout  反向dropout，来训练神经网络

1、通常的做法是设置一个阈值 keep-prob---为这一层某些隐藏的节点归为0的概率

2、通过numpy生成一个随机矩阵--表示这一层那些节点被隐藏

3、使用这一层的激活之后的结果与d3相乘，可以把某些节点最后结果归为0

4、因为a3有的节点变为0了，为了不影响后面一层的期望，将 a3/keep-prob补齐期望

![image-20250428090251399](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931438.png)

为什么dropout可以生效

dropout随机的丢弃一些单元，防止后面的计算过度的依赖某一单元

如果某一层的矩阵非常大，可以设置较小的阈值，比如右下的图

但是设置dropout使得我们没有办法设置明确的损失函数，所以画不出损失函数与迭代次数的关系图，所以刚开始的时候可以关闭dropout，将keep-prob设置为1，确保损失函数下降然后再打开dropout

dropout通常使用在计算机视觉之中

![image-20250428091634950](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931439.png)

其他正则化的方法

数据扩增 通过旋转图片/裁剪/扭曲的方法增大数据集

![image-20250428093025398](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931440.png)

早停止训练，查看损失函数与迭代次数的图，如果在某一时刻交叉集误差变大那么说明可以停止训练了

![image-20250428092858308](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931441.png)

##### 归一化处理

将数据压缩到一个更小的范围内，注意训练集和测试集都使用同一个平均值和方差进行缩放

![image-20250428093726735](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931442.png)

经过归一化处理之后，更容易训练神经网络，损失函数与参数图更圆

![image-20250428093825347](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931443.png)

##### 梯度消失/爆炸

根据设定的参数不同，如果w矩阵大于单位阵，那么它最后所产生的y是会变的非常大的，如果参数w矩阵要比单位阵小很多，最后的y会变得非常的小，所以就会产生梯度爆炸/消失

![image-20250428094607817](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931444.png)

解决梯度爆炸/消失

如果数据非常大，想要输出的y值较小，可以使用较小的w，解决方法是在初始化参数的时候让参数乘不同的值

![image-20250428095510559](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931445.png)

梯度数值的逼近

使用双边误差比单边误差的精度要高，比如双边逼近的误差是0.0001，单边误差是0.03

![image-20250428100343945](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931446.png)

梯度检验

将w矩阵变成一个大的向量，然后将w，b连接起来，将所有的参数构成一个大个向量，用theta表示求出j

然后求出所有参数的梯度，他们跟初始化的参数有着相同的维度，然后将他们按照上面的办法进行相连，构成dtheta

![image-20250428102830170](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931447.png)

使用双边误差--计算误差--使用欧式距离的方式计算

theta1，theta2对应的是一/二层的w，b，

![image-20250428102946955](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931448.png)

梯度检测的注意事项

1、不在训练模型中运行，只在调试模型的时候使用

2、如果算法梯度检验失败，根据误差尝试修订bug

3、因为计算的是损失函数，损失函数有的时候会存在正则化

4、不能和dropout一起使用

![image-20250428104412932](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931449.png)

##### bath/mini batch

如果有一个大的数据集，训练的不需要一次遍历所有的数据集然后再进行梯度下降，只需要把数据集分割成小的，对每一个小的数据集进行训练，进行梯度下降

![image-20250429091432157](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931450.png)

![image-20250429091556305](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931451.png)

mini batch的损失函数图像

不会一直变小但是大趋势是变小的

![image-20250429092627036](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931452.png)

选取mini batch的大小

如果选取的为m，那么就相当于一次训练一整个数据集

如果选取为1，没有用到向量的加速作用

建议选取1~m之间的值

![image-20250429092826050](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931453.png)

##### 指数加权平均

![image-20250429094606055](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931454.png)

根据参数的不同，β vt所代表的意思不同

如果β等于0.9，vt计算的是前10天的加权移动平均值

![image-20250429094622972](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931455.png)

具体细节

为什么β等于0.9就是平均10天的温度，因为可以把等式展开，0.9的10次方已经很小了，后面的计算可以忽略

相同的β等于0.98，相当于平均了50天的温度

![image-20250429101255298](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931456.png)

偏差修正

在计算的早期，预测是不精准的，所以使用偏差修正，比如下图右边，好处--只在预测的早期有用，到预测的后期β的t次方就会变得很小，不起作用了，并且预测精准

![image-20250429102353753](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931457.png)

momentum 梯度下降法

使用了加权移动平均，如果一个模型的损失函数上下摇摆，总体上是趋于更小的值，可以使用momentum来优化，

减少上下浮动，加快左右移动，就行上面求温度的方法一样

可以使用一个大的学习率来增快学习时间

![image-20250429162802898](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931458.png)

细节

通常使用（1-β的版本）

![image-20250429163034286](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931459.png)

RMSprop

加快梯度下降的算法，如图所示，纵轴代表b，横轴代表w，我们想加快w，减少b，使用rmsporp算法

计算公式如下，计算sdw，sdb，然后更新参数，注意除的时候不能除以0，要加上一个小的数

因为dw的值较大，平方之后值更大，在更新的时候除以一个大数，w的变化就会变小，同理b也是

![image-20250429164158484](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931460.png)

adam算法

更加通用的算法，结合momentum以及rmsprop

![image-20250429165446621](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931461.png)

超级参数的选择

学习率需要自己实验，下面几个是推荐

![image-20250429165532951](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931462.png)

##### 学习率衰减

在训练刚开始的时候可以承受较大的移动，训练越到最后要想收敛的话可以减少学习率，让他的摆动变小

![image-20250429170259676](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931463.png)

学习率衰减的公式  alpha0为初始学习率，epoch-num为迭代次数，decay-rate 下降率

![image-20250429170419970](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931464.png)

其他降低学习率的方法

![image-20250429170620385](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931465.png)

### 超参数

![image-20250430090714044](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931466.png)

#### 超参数的选择方法

1、使用随机取点的方式实验那个点表现比较好

![image-20250430090753939](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931467.png)

取点的过程由粗糙到精细，比如在一个大的地方有几个点表现较好，那么就缩小范围，在这几个点的周围密集取点，在进行比较那个区域的表现较好，逐渐缩小取点范围

![image-20250430090816338](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931468.png)

#### 为超参数选择合适的范围

对数轴取值--对于学习率

如果使用数轴在0.0001-1之间取值，发现大部分都会在0.1-1之间取，为了保证均匀性，我们使用对数轴取点，

对于对数数轴进行划分，分别在这几个区间内取值，rand（）是均匀分布

![image-20250430091731064](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931469.png)

β的选则

通常计算1-β，然后在对数轴上均匀取值

![image-20250430093222449](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931470.png)

超参数训练实践

两种方式1、熊猫训练方式--计算资源不足，训练一个模型，通过观察模型的表现调整超参数

2、鱼子酱训练方式--计算资源充足，一次训练多个模型，使用不同的超参数

![image-20250430094107090](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931471.png)

### batch norm

正则化输入对模型的训练有帮助，那么能不能将每一层的输出都标准化--batch norm

![image-20250430100240609](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931472.png)

实现方式

将所有层的z全部标准化，如果你不想每一层的输出都是均值为0，方差为1，那么可以使用参数修改，γ，β

![image-20250430100336022](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931473.png)

batch norm融入到深度学习之中

每一层都将z均值归一化

![image-20250501094636093](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931474.png)

batch norm和mini batch 结合

可以不使用参数b，因为b在每一次均值化的时候减去均值被消除了

![image-20250501094744624](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931475.png)

与mini batch的总结

![image-20250501094827732](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931477.png)

为什么batch norm 管用

如果数据过分依赖前面产生的计算结果就会出现covariate shift

![image-20250501213428867](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931478.png)

所以batch norm 将前面产生的数据正则化，将方差，期望缩放到一个固定的位置，有利于减少对前面参数的影响，更好的训练

![image-20250501213544473](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931479.png)

![image-20250501213707209](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931480.png)

测试中的batch norm

在测试的时候，u和方差是计算一整个数据集，可以使用加权移动平均来计算每个mini batch 得出

![image-20250501214402333](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931481.png)

### softmax激活函数

用于多分类，将使用全部的输出数据，进行计算，输出的结果是属于那个类的概率

![image-20250501220245831](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931482.png)

## 模型建议

### 正交化

正交化--每一个按钮调节一个东西，比如下图有四种不同的情况，每个情况都有一个或者几个按钮（解决方法）来控制，他们互不影响

![image-20250510091841620](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931483.png)

### 单一数字评估指标

使用一个指标来评价模型，而不是多个，本例使用的是f1分数判断分类器的好坏

![image-20250510093041801](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931484.png)

使用平均值来判断模型的好坏

![image-20250510093125001](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931485.png)

### 满足和优化指标

当有多个指标的时候，可以选择一个作为优化指标，其他的都作为满足指标

比如，本图，选择准确率作为优化指标，不断的去提升这个指标，运行时间作为满足指标（小于100ms）

![image-20250510093859773](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931486.png)

### 设定开发集和训练集

开发集和测试集要抽取所有类型的数据，迭代方向要朝着最小化开发集损失函数以及单个数字评估指标靠齐

开发集和测试集要来自同一分布

![image-20250510095157927](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931487.png)



### 数据的划分

少量的数据可以进行60/20/20的划分

大数据可以进行98/1/1的划分

![image-20250510101300922](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931488.png)

要设定大的测试集，以至于可以使模型有很大的置信度  通常可能远远小于整个数据集的30%，比如上图的1%

![image-20250510101424582](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931489.png)

### 什么时候改变指标

当一个指标不能评价出模型的好坏的时候需要更换指标

![image-20250510104658791](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931490.png)

如果一个模型在开发集和测试集表现的很好，但是在实际的部署中表现的不好，可能需要更换指标以及开发集和测试集

![image-20250510104759537](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931491.png)

可避免偏差

人类误差与训练误差的差值



![image-20250510112313633](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931492.png)

### 通过一些方式来改善你的模型

![image-20250510170501904](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931493.png)

### 错误分析

对于一个模型，选取开发集中错误的100个例子，查看其中被错误标记的狗的数量占多少，如果占的比较多，可以处理一下，这样模型的表现会有一个显著的提升

![image-20250510171733710](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931494.png)

在评估的过程中可能会出现其他的问题，可以列一个表，扫描错误的例子，统计每个类别其中错误比列，根据比例考虑是否要处理这个问题

![image-20250510171855868](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931495.png)

是否修改标注错误的数据--看所占的比例

![image-20250510175815153](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931496.png)

建议，

1、如果开发集修改了标签那么测试集也要进行修改

2、考虑算法分析的正确例子和错误例子--很少这么做

3、训练集可以不做修改

![image-20250510175834020](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931497.png)

### 快速搭建一个神经网络并快速迭代

首先设定开发集测试集一个指标，建立一个简单的且快速的网络，使用偏差/方差分析，以及错误分析，分析下一步应该朝着什么方向优化

![image-20250510213252070](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931498.png)

### 在不同的划分上进行训练并测试

大部分的数据是图片很清晰的，但是实际情况是模糊的，所以数据集划分的时候，将开发集与测试集都变成实际的数据，并且给训练集分一些实际的数据

![image-20250510214653747](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931499.png)

开发集/测试集与训练集的数据大部分不相同时，判断偏差和方差

设定一个训练-开发集，这个数据集不用来训练模型，并且这个训练-开发集与训练集的数据分布相同

可以在训练集上训练模型，然后尝试计算训练开发集的误差，比较人类误差与训练误差判断是否有高偏差，

比较训练误差与训练开发误差判断是否有高方差，比较训练开发误差与开发误差判断是否有数据不匹配的问题

![image-20250510220323855](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931500.png)

根据误差判断偏差、方差、数据不匹配、过拟合程度

![image-20250511173133235](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931501.png)

解决数据不匹配问题

1、可以让训练数据集更像开发集

2、收集更多的与开发集和测试集相似的数据

注意人工合成数据，会导致过拟合问题，因为在很大的可选范围内，只进行了一部分数据的模拟，大量的此类型的数据，会导致神经网络过拟合

![image-20250511174345521](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931502.png)

### 迁移学习

1、迁移学习要点：两个学习的输入是相同的

2、任务a的数据要比b大的多

3、在a中学习很多低的特征有助于b的学习

![image-20250511213534086](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931503.png)

可以在网络上下载别人的网络模型以及相关参数，然后根据自己的需要修改输出层，根据数据量的多少可以冻结不同的层，训练不同的层，如果数据量越少推荐只训练最后一层

![image-20250514105444173](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931504.png)

### 多任务学习

可以使用一个神经网络同时完成许多事情，比如查看一张图片里面有没有行人、车、停车牌、信号灯，使用logistics损失函数

并且如果图片的标签有一部分没有标记也可以使用这样的数据集进行训练

![image-20250511215240641](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931505.png)

什么时候起作用

1、识别的其他东西的特征，也可以用于识别另一个物品

2、每个任务的数据量相似

3、可以训练一个比较大神经网络

![image-20250511215422058](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931506.png)

### 端到端的学习

可以使用：有大量相关的数据

比如翻译任务，不需要经过很多的步骤再翻译，如果有大量的信息（x，y）英语到法语的直接数据对，可以使用端到端的神经网络

不可以使用：从一个x光判断孩子的年龄，直接从图片判断年龄效果不太好，可以把复杂的问题分解成几个问题来解决

![image-20250511220900351](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931507.png)

#### 优点缺点

优点：1、可以让数据说话，让数据自己完成网络，不让人类干预

2、更少的使用人工设计的组件

缺点：需要大量的数据，不一定比人工设置的组件管用

使用端到端的神经网络关键就是有没有大量的x，y映射的数据

![image-20250512094033348](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931508.png)

## 计算机视觉

### 卷积

左边是一个图片，中间是核/滤波器，可以用来做垂直检验，经过卷积运算得到右边的矩阵，看下面的效果图可以看到中间变成白色的了，检查出来垂直的地方

![image-20250512212718880](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931509.png)

也可以将过滤器旋转让他变成水平检测，

正边--求出的结果是两边暗中间亮，负边则是相反的

![image-20250513094021183](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931510.png)

不同的滤波器，sobel、scharr，也可以将滤波器修改为参数，通过反向传播的方式进行学习

![image-20250513094151701](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931511.png)

#### padding

卷积会有两个问题：1、卷积之后图片变小，2、会丢弃一些边缘的信息

所以使用padding来解决这个问题，可以再图片周围做一些填充，通常使用0进行填充，这样可以解决上面两个问题

![image-20250513095238971](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931512.png)

所以诞生两种卷积：1、valid卷积，每次卷积之后图片变小，可以根据公式计算卷积之后的图片的大小n-f+1，n是原图片大小，f是滤波器，

2、same卷积，卷积之后图片的大小不变，相同的可以根据公式计算卷积之后的图片的大小n+2*p-f+1，n是原图片大小，f是滤波器，p使填充的大小



推荐只使用奇数的滤波器

![image-20250513095413192](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931513.png)

#### stride

步长，每次框移动的步长，如果步长不为1，卷积直接的图像大小为n+2p-f/s+1，n为原图像大小，p为填充，f使滤波器大小，s是步长，如果算出来的结果不为整数要向下取整。如果滤波器移动到了图像外面，那么就不进行计算

![image-20250513100929021](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931514.png)

#### 3d卷积

图片是有三个通道的滤波器也要有三个通道，最后得出的是只有一个通道的图片，计算方式跟单个通道的滤波器一样，不过最后计算结果要加起来，

![image-20250513102903677](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931515.png)

多个滤波器

想要不同的功能可以选择不同的滤波器，第一个适用于垂直检测，第二个用于水平边缘检测，最后得到的矩阵堆叠在一起

原来是6*6 *3的图片，经过两个3* * 3 * 3的滤波器，变成了4 * 4 *2

![image-20250513103046201](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931516.png)

#### 单个卷积网络

一层卷积网络的运行方式是，设定几个滤波器，对于每个滤波器进行计算，得到一个矩阵，给矩阵添加偏置和使用激活函数，得到最后的结果，再把结果堆叠起来，

![image-20250513110530054](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931517.png)

10个3 * 3 * 3的滤波器的参数个数为280个，因为每一个滤波器都要加上一个偏置

![image-20250513110721296](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931518.png)

卷积网络的一些标记

![image-20250513110819023](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931519.png)

使用标记来标记卷积神经网络

![image-20250513112228703](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931520.png)

#### 池化层

最大池化层：求的一个区域的最大值。f=2，步长=2，没有任何参数

![image-20250513113455104](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931521.png)

平均池化层

求一个区域的平均值

![image-20250513113549128](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931522.png)

总结

需要选定超参数，滤波器的大小，步长，以及使用什么池化层，计算池化之后的大小与卷积层的一样，并且池化层没有参数

![image-20250513113612598](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931523.png)

#### 构建一个神经网络

使用卷积层、池化层、全连接层构建一个神经网络

![image-20250513144824806](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931524.png)

网络中的一些信息

![image-20250513144853718](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931525.png)

#### 为什么使用卷积

使用卷积使得网络训练所使用参数很少，1、因为卷积可以实现参数共享，对于右部分比较有效的滤波器，对于左边也比较有效果

2、稀疏连接：矩阵左上角的一个数值，只与原矩阵左边9个数值有关，所以只需要连接9个和一个就可以

![image-20250513145916107](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931526.png)

构建神经网络，随机初始化参数，使用损失函数反向传播，来减少损失函数。

![image-20250513150154477](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931527.png)

### 网络模型

#### 经典模型

lenet-2

![image-20250513155559222](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931528.png)

alexnet

![image-20250513155616197](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931529.png)

vgg-16

![image-20250513155700246](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931530.png)

#### 残差网络

网络的传递不在是从头到尾，而是中间可以传递其他的值，比如al可以直接传递到al+1层与z[l+2]做一个加法，在使用激活函数这就是远眺连接，可以用来训练更深的网络，有助于解决梯度消失/梯度爆炸，并且保证网络良好的性能

![image-20250513161600888](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931531.png)

残差网络

![image-20250513161953452](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931532.png)

##### 残差网络为什么有用

![image-20250513164058325](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931533.png)

![image-20250513164258902](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931534.png)

#### 1*1的卷积

![image-20250513165412814](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931535.png)

可以用来压缩或者扩大信道

![image-20250513165433754](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931536.png)

#### inception网络

不需要自己指定核的大小，inception可以帮助你选择

![image-20250514101205512](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931537.png)

可以使用1 * 1的卷积网络做计算的压缩

不使用1 * 1卷积 需要计算1.2亿次

![image-20250514101301659](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931538.png)

使用之后变成原来的十分之一，使用1 * 1的卷积构造出瓶颈层压缩计算量

![image-20250514101336327](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931539.png)

inception网络的基础模块

![image-20250514102936455](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931540.png)

通过inception模块构成inception网络

![image-20250514103018257](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931541.png)

#### 数据增强

通过镜像、随机裁剪、选转、扭曲、修改rgb的值来增加数据集

![image-20250514110634706](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931542.png)

### 目标定位

在图像中定位目标物体

![image-20250514145105909](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931543.png)

需要修改输出

输出为图片中的物体属于哪一个类，以及物体在图片中的坐标

![image-20250514145147482](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931544.png)

y的定义

y的定义包括pc--是否存在目标，不存在为0，目标物体的坐标，属于那个类

坐标 b_x,b_y是物体的中心点，b_h,b_w是物体的长宽

误差为左边公式

![image-20250514145230364](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931545.png)

#### 特征点检测

如果想要图片中的眼角位置，下颚状态，可以添加相对应的数据，来实现预测

![image-20250514150008559](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931547.png)

#### 目标检测

如果想检测图片中是否右汽车，可以对图片进行裁剪，使汽车占满图片，然后训练网络。使用滑动窗口的方法

![image-20250514150703030](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931548.png)

使用滑动窗口的方法，每次使用一个小方块从图片的左上角开始检测，取一小块图片放到神经网络之中，如果有汽车就标记为1，

然后扩大方块再次进行检测，需要规定步长，步长太小就会产生过多的计算量

![image-20250514150807904](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931549.png)

#### 卷积的滑动窗口实现

将全连接层转变为卷积层

![image-20250514152124252](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931550.png)

使用卷积进行滑动窗口--可以一次性输出图片里面有没有想要的目标

1、第一个卷积是一个输出是一个1 * 1 * 4的，可以看这个区域里面有没有想要的目标

2、最后的输出是2 * 2 * 4，可以查看4个区域有没有目标

![image-20250514152151342](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931551.png)

#### yolo

单个卷积的缺点是不能很准确的标出框，所以使用yolo算法

将一个图片分成很多分，比如本图的3 * 3，每一个区域都对应着一个y，y是target，其中有p_c,物体坐标，以及各种类c_1..

如果一个区域有某个物体，物体的中心点在这个区域，就将物体划分到这个区域之中，利用上面的卷积滑动窗口来计算，一次实现标记区域之中是否有想要的目标

![image-20250514154140726](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931552.png)

注意b_x,b_y通常小于1，那两个可能大于1，因为是物体占小区域的比例，b_x,b_y是中心

![image-20250514154630113](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931553.png)

##### 交并比

查看算法给出的框与自己定位的框交集和并集的比值，如果值较大说明算法给出的越精准

![image-20250514161633694](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931554.png)

##### 非极大值抑制

物体检测会出现对于一个物体检查出多次的情况，使用非极大值抑制可以解决

选出p_c最大值，然后根据交并比将别的框舍去

![image-20250514162730499](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931555.png)

细节

一共是19 * 19个块，对于每一个块都一个输出，

先舍弃pc值小于0.6的框，在剩余的框中挑选最大的pc值，丢弃与选出框交并比大于等于0.5的框

![image-20250514162814871](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931556.png)

##### anchor box

解决同一框里面出现两个检查物体

为每一个格子绑定两个anchor box

![image-20250514164328235](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931557.png)

对于检测的物体，每一个格子都有两个anchor box，根据检测出来的物体，计算框与anchor box的交并比选择一个最大的

![image-20250514164356482](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931558.png)

![image-20250514164534938](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931559.png)

##### yolo合

训练，为每一个框绑定两个anchor box

![image-20250514165506259](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931560.png)

预测

![image-20250514165536394](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931561.png)

使用非最大值抑制

![image-20250514165559801](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931562.png)

#### 候选区域

使用图像分割算法将图像进行色域的分割，然后再这些颜色上面使用物体检测算法

![image-20250514170250943](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931563.png)

### 人脸识别

![image-20250515213016264](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931564.png)

#### 一次学习

一次学习每个人的数据照片都只有一个，希望通过一张照片来进行人脸识别

使用similarity解决一次学习的问题，d（img1，img2），来计算两个照片是否相似，如果小于一个阈值那么说这两个照片就是一个



![image-20250515213530405](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931565.png)

#### Siamese network

使用网络解决人脸识别，把一张照片输入到网络，最后输出特征，然后根据特征计算两张照片的相似程度

![image-20250515214141715](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931566.png)

如果两张图片的相似程度d很小，那么这两张图片就是同一个人，相反则不是

![image-20250515214250114](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931567.png)

##### triplet损失函数

三元组损失函数，需要一次识别三张图片，一张原图片，一张与原图片是一个人的图片，一张另一个人的图片，对于这三个图片都放到网络之中输出3组特征，要使得anchor与positive的距离加上alpha小于等于anchor与negative的距离，alpha是一个超参数，为了使anchor与negative的距离更远

![image-20250515215345143](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931568.png)

损失函数，训练集是10k张图片1k个人的，训练集需要每个人有多种图片，训练完网络之后可以去解决一次学习的问题

损失函数取最大值，如果前一个小于0，那么距离就是很大，这样目的达到了，就使得损失函数最小为0，

![image-20250515215602077](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931569.png)

如何选择apn

尽量选择大的距离的图片，那么模型更加容易训练

![image-20250515220717001](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931570.png)

可以将模型转换成sigmoid输出，最后损失函数是下面的这个，训练的时候图片可以是两张，如果是一个人那么就输出1，如果不是就输出0

![image-20250516150912751](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931571.png)

### 神经风格迁移

![image-20250516151509708](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931572.png)

#### 损失函数

包括内容损失函数和风格损失函数，每一个损失都有一个超参数控制

![image-20250516153501294](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931573.png)

##### 内容损失

![image-20250516154629169](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931574.png)

##### 风格损失

计算的是不同通道的相关系数

![image-20250516161244751](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931575.png)

公式

![image-20250516161322200](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931576.png)

可以对各层使用这个损失函数就是第二个公式

![image-20250516161455249](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931577.png)

大概流程

随机初始化图片，然后根据代价函数逐渐的优化成想要的风格

![image-20250516153528110](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931578.png)

从二维扩展到一维和三维

![image-20250516162955186](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931579.png)

![image-20250516163009539](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931580.png)

## 序列问题

### 序列数据

![image-20250516165227173](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931581.png)

#### 数学符号

用一些符号代表信息

![image-20250516175715767](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931582.png)

如何表示词

使用字典的方式表示，根据词在字典中的位置使用one hot编码表示

![image-20250516175751270](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931583.png)

### 循环神经网络

为什么不使用传统网络

会出现两个问题，1、输入输出在不同的例子之中是不同的长度，如果使用传统的网络会有一个固定输入，如果不够还需要更改网络结构，2、每个节点不会共享参数，如果每个输入使用的是词字典的输入，那么输入就会很大，与第一层连接就会产生很多的参数。

![image-20250517092402202](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931584.png)

循环神经网络

每一个输入都对应输出，并且每一个输出不知需要这一个输入还需要前面的输出，造成的问题是每个输出只使用前面的计算结果，而不使用后面的量，在BRNN 中会解决这个问题

注意第一层的输出也需要前面的出的激活值，不过这个是认为人自己构成的，通常使用0向量表示

最右边的图通常出现在教材或者论文之中，展示是左边的图

注意参数是共享的

![image-20250517092621859](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931585.png)

前向传播

a的激活函数是tanh/rulu，通常是tanh，y的激活函数根据输出的不同选择不同的激活函数

本例是判断单词是不是人的名字，所以y的激活函数是sigmoid

![image-20250517092925230](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931586.png)

对于rnn计算的公式的简化

可以将waa，wax合成一个行矩阵，并且将at-1与xt合成一个列矩阵，然后进行乘，可以得出结果，这样的输入就不是两个矩阵，而是一个

![image-20250517093029722](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931587.png)

rnn的前向传播与反向传播

本例的损失函数是二分类的损失函数，因为输出是0，1，判断是单词是不是人的名字

对于每一个输出的单词都有一个损失，到最后将损失全部加起来

![image-20250517094503307](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931588.png)

不同类型的RNN 网络

![image-20250517102620778](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931589.png)

#### 语言模型

训练集是一个大的语料库，输入的是单词的one hot编码，并且下一个输入是需要上一个的正确的单词

![image-20250517111552696](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931590.png)

完整模型

y1的输出是字典中所有单词第一次出现的概率，y2的输出是在第一个是cats的情况下其他单词第二个输出的概率

损失函数就是softmax的损失函数，把每一个的输出加起来在计算，然后进行反向传播

并且x2的输入是y1的预测值

![image-20250517111705018](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931591.png)

#### 采样

对于一个已经训练的模型进行采样，

![image-20250517184026119](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931592.png)

除了单词语言模型还有字符语言模型，这样输出就不会有unkonw了

![image-20250517184132414](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931593.png)

#### 问题

循环神经网络出现的问题是不能解决长期依赖的问题，比如一个神经网络很深，前面出现cat或者cats，中间有相当多的数据，后面的was/were可能不会根据cat/cats进行输出，只能根据was/were前面几个输出预测

同样的神经网络很深会产生梯度消失/梯度爆炸的问题

梯度消失问题可以用GRN来解决

梯度爆炸使用梯度剪裁的方式解决--还不会

![image-20250517184907441](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931594.png)

##### RNN unit

![image-20250517191851981](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931595.png)

##### GRU unit

c~t表示带更新的值，gammau表示一个权重，ct表示经过GRU之后的值，

根据gamma来看ct是否需要更新

可以解决长时间依赖问题以及梯度消失

![image-20250517192417861](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931596.png)

完整的GRU unit

![image-20250517192642177](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931597.png)

#### LSTM网络  

长短时记忆模型

![image-20250517215243730](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931598.png)

![image-20250517215311616](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931599.png)

#### BRNN网络

双向RNN网络，除了前部分单词的激活值还有后面单词的激活值，考虑整个句子

![image-20250517220155033](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931600.png)

#### 深层的循环网络

通常是一层，可以加上几层，注意下方例子如何计算的

![image-20250517221131252](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931601.png)

可以在单独的一层后面多加几层，但是他们之间没有连接，这也是比较常见的

![image-20250517221223149](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931602.png)

### 词嵌入

使用特征来表示单词，每个特征取值是-1到1，假如有300个特征，每个词都是300维的向量，通过词嵌入可以更好的发现词与词之间的相似性

![image-20250518104855815](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931603.png)

可视化词嵌入

通过t-sne算法可视化词嵌入，将高维的数据映射到二维空间

![image-20250518105025103](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931604.png)

词嵌入的使用

来分辨单词是不是人名，使用词嵌入，每一个单词都是一个300维向量，使用rnn来判断，如果进入一个新的词比如apple farmer，rnn会根据词嵌入，比如orange与apple的词嵌入很相似，来判断robert lin 是一个人名，同样的第三个例子，也是根据词嵌入比较相似来判断

![image-20250521092728198](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931605.png)

词嵌入的迁移学习

可以下载网络上以及训练好的词嵌入模型，如果有大量的标签数据，可以使用自己的词嵌入在模型上进行微调，然后使用微调好的模型进行训练

![image-20250521092959540](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931606.png)

#### 类比推理

根据词嵌入得到的向量进行类比推理--比如 相对于man对应着woman，king对应着什么

使用man的向量将去woman的特征向量会得到一个向量，数字绝对值最大的就是他们相差的最大的 

同样的使用king-queue 得到的与man的那个差不多，根据这个可以判断king对应着什么

![image-20250521094624024](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931607.png)

可视化

通常使用一个公式来表示 最下面的公式

![image-20250521094848951](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931608.png)

使用余弦相似度来计算

![image-20250521094951108](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931609.png)

#### 词嵌入矩阵

有一个字典有10000个字，每个字都有300维的特征，这就是一个词嵌入矩阵，通过词嵌入矩阵乘某一个字典的one-hot编码会得到这个词的词嵌入向量

![image-20250521095913149](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931610.png)

#### 学习词嵌入

通过将单词one-hot表示，随机生成词嵌入矩阵e，使用e乘单词one-hot编码得到单词的词嵌入矩阵，将单词的词嵌入矩阵放到神经网络之中来预测最后目标单词是什么，根据预测结果进行梯度下降

![image-20250521101726648](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931611.png)

预测的目标单词的话使用目标单词前4个单词就可以，如果学习词嵌入矩阵，可以使用下面几个方法

![image-20250521101910798](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931612.png)

##### word2vec模型

skip gram--随机选取文本c 在从文本c周围给定范围的内选取目标词，组成训练数据，根据给出文本预测目标文本

模型，有文本词汇和目标词汇字典里面有10000个词汇，每个词汇使用one-hot编码，根据生成的E来求文本单词的词嵌入矩阵，把词嵌入放入softmax里面，求出yhat，根据损失函数进行优化

softmax输出是在c文本条件下输出t文本的概率，

![image-20250521104849411](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931613.png)

产生的问题是，词汇的单词太多了，使用softmax计算会消耗很多的时间，想方法进行优化

以及如何选取c的问题

![image-20250521105152496](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931614.png)

##### 负采样

对于一个文本词，选取它附近的词，作为一个正例子，然后对于这个文本词在字典中选取选取词汇，组成负例子，注意字典中如果选到与文本词附近的词也不用担心，还是把它标记成负例子，然后对于文本词提取它的词嵌入矩阵，放到二分类的模型里面，训练模型，模型的结果是在0-1之间的概率，解释为在c条件下选取该词的概率大小，训练k+1个分类器，k是负样本的个数，1是正样本的个数

![image-20250521111418321](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931615.png)

如何选取负例子

根据下面的公式进行选择，f（wi）是词汇在文本文件中出现的频率，根据它的3/4次方与全部词汇3/4次方的比值，进行选取每个例子

![image-20250521111930333](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931616.png)

##### glove

这个算法是找xij，i在j的上下文出现的次数，根据条件不同xij不同，比如条件是上下文10个单词之内，或者i在j之前出现

![image-20250521145856196](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931617.png)

模型为  f（xij）是权重，防止xij=0，并且也能处理不同频率单词的权重

![image-20250521150033975](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931618.png)

#### 情感分类

一个简单情感分类模型，获取每一个单词的词嵌入向量，平均一下，放到softmax里面输出1-5个星，但是有一个缺点是不考虑意思，比如一句话里面有好多的good，但是这句话之前有一个lacking，是不好的，所以没办法解决这个问题

![image-20250521151535829](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931619.png)

rnn模型

使用rnn可以很好的解决这个问题，

![image-20250521151731622](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931621.png)

### seq2seq模型

机器翻译，输入的是法语输入到编码器，然后在解码器进行输出

![image-20250522095457667](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931622.png)

imag2seq模型

通过卷积网络模型将图片的特征捕获然后放到rnn网络中输出相对于的描述

![image-20250522095553384](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931623.png)

#### 条件语言模型

语言模型的输入a0是一个0向量，而机器翻译的的输入是前面输入句子计算而来的所以叫做田间语言模型

![image-20250522100628931](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931624.png)

翻译模型需要解决的问题

如果使用贪心搜索的话假如输入jane is 两个词在字典里面is 与going的概率要大于 is 与visiting的概率，并且如果字典中单词数量较多，比如有10000个，翻译语句长度有10，则一共有10000的10次方中结果，计算比较大，所以要采取一种方法进行解决

![image-20250522100758050](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931625.png)

解决上述问题

使用一种方法使得在法语输入条件下使得句子找到相对较好的翻译的概率变大

![image-20250522101053245](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931626.png)

#### 定向搜索/束搜索

使用定向搜索解决上述问题

1、设置束宽为3，表示每次选取3个概率最大的单词，将法语输入到编码器之中，在解码器输出第单词，输出的单词是根据法语的输入然后经过10000个softmax输出的3个概率最大单词

![image-20250522102424827](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931627.png)

2、根据前面输出的单词，在法语输入以及前面单词的条件下输出概率最大的单词，因为束宽为3，词典的长度为10000，所以输出的单词概率总数为30000，要在30000中选取3个

![image-20250522102631701](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931628.png)

第三步跟第二步的方式一样，在前两个单词以及x的输入的条件下输出概率最大的单词，在30000个概率中挑选最大的3个，一直到最后整个句子的结束

![image-20250522102841333](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931629.png)

##### 改进定向搜索

定向搜索的目标是最大化第一个公式，但是概率的值基本上都是远小于1的数，会导致越乘越小，这样最会得到的数字可能会产生向下溢出，并且会偏向翻译字数短的句子，为了数字下溢的问题使用log方式，在连乘前面加一个log，变成相加，但是log也有偏向翻译短的句子，所以使用长度归一化的处理，在前面除一个系数，带有alpha的指数，

![image-20250522150409576](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931630.png)

##### 束宽的选择

对于不同的产品选择不同的束宽，束宽越大产生的效果越好，但是运行起来比较慢，内存占有率大，束宽越小则相反

![image-20250522151155654](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931631.png)

##### 束搜索的误差分析

算法表现的不好，需要对开发集进行误差分析

人类翻译的是y* 算法翻译的是yhat，计算p（y   ***|x）与p（yhat|x）的概率，对于一下那个大，如果y*** *大，但是束搜索选择yhat，这就说明是束搜索的错误，如果是y  * 的翻译比较好，但是rnn预测yhat的概率大，那么就是rnn出错误了

![image-20250522152606797](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931632.png)

分析出错误每一个句子，查看是什么错误，最后计算比例，根据比例做相对应的处理

![image-20250522152958484](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931633.png)

##### bleu得分

评估机器翻译，一个法语句子通常可以翻译成几个英文句子，并且这几个翻译都是较好的，对于这个有较多比较好的答案的时候如何对机器翻译进行评分--bleu进行评分

bleu可以进行单个单词评分以及多个单词评分公式为（参考1/2出现单词的最大次数）/机器翻译总句子的长度

![image-20250522154928221](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931634.png)

二元单词组得分计算如下  countclip是参考1/2出现单词对次数最大的哪一个  count是翻译单词对的次数

![image-20250522155231308](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931635.png)

可以计算多的单词组然后进行平均算得分，但是有一个缺点就是倾向于输出短的翻译，短的翻译命中参考翻译的命中率较高，所以要增加一个惩罚项就是bp

![image-20250522155330115](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931636.png)

### 注意力模型

机器翻译是读取一个完整的句子然后再进行翻译，缺点就是对于长句子来说翻译的情况不好，所以采用注意力机制来解决

注意力机制输出只关心一部分输入的句子，使得结果更像人类翻译

![image-20250522161421300](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931637.png)

注意力机制的直观理解

使用双向的rnn模型，将单词输入到模型之中，计算得到输入下一步的权重，再下一层需要一个激活s0输入，输出第一个单词时需要考虑观察周围单词的注意力权重alpha<1,1>、alpha<1,2>、alpha<1,3>、、、、第一个数字是翻译输出第几个单词，第二个数字代表是第几个输入，alpha的意思是输出第一个，需要考虑第一个输入的权重，根据权重进行计算。

输出第二的时候也需要权重  alpha<2,1>、alpha<2,2>、、、、并且也需要输出的第一个单词作为输入进行计算

![image-20250522161530230](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931638.png)

细节 注意c的计算方法，

在输出一个单词的时候，所有时间步的权重加起来等于1

![image-20250522165336297](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931639.png)

如何计算权重

![image-20250522165447599](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931640.png)

### 语音辨别

![image-20250522170812299](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931641.png)

使用注意力机制来进行语音辨别

输入的是音谱图的特征

![image-20250522170843226](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931642.png)

使用ctc来进行语音辨别

输出的长度是固定的，可以重复输出，然后ctc可以将重复的字符合并

![image-20250522170902656](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931643.png)

### 触发字检测

使用rnn或者其他的模型，输入音频的特征，会输出0/1，在关键字部分输出1，其他部分输出0，缺点就是输出0太多了，可以在下一次输出0之前全部输出1。

![image-20250522171551590](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/deeplearning/202505230931644.png)