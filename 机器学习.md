# 机器学习

## 序幕

### 监督学习

![image-20250405161015707](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917787.png)

线性回归

![image-20250405161120065](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917788.png)

逻辑回归

![image-20250405161213391](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917789.png)

### 无监督学习

![image-20250405161320657](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917790.png)

## 线性回归

![image-20250405161506827](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917791.png)

数据集

![image-20250405161551926](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917792.png)

基础模型

![image-20250405161706399](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917793.png)

损失函数  

m是样本的个数，乘2是因为求导的时候可以消除，便于计算

![image-20250405161835211](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917794.png)

损失函数的可视化理解

![image-20250405161931092](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917795.png)

二维变量线性回归的损失函数图像

![image-20250405162047594](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917796.png)

损失函数等高线图

![image-20250405162127792](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917797.png)

梯度下降--找到损失函数的局部最小值

![image-20250405162337524](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917798.png)

梯度下降的实现方式

同时更新参数  学习率永远是正数

![image-20250405162450512](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917799.png)

梯度下降的直观理解  j是损失函数，改变参数w使得损失函数的值最小

如果损失函数较大，则偏导就很大，梯度下降的步子也比较大

![image-20250405162658430](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917800.png)

学习率 α --是一个固定的值

学习率如果太小，参数的每一步变化将非常小，使得模型收敛需要经过很多轮。

学习率如果太大，模型可能无法收敛

![image-20250405162943364](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917801.png)

线性回归中实现梯度下降

![image-20250405163323772](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917802.png)

推导

![image-20250405163353304](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917803.png)

带回到公式中

![image-20250405163417809](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917804.png)

多个特征的线性回归

![image-20250405163820846](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917805.png)

模型变成

![image-20250405163854214](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917806.png)

可以用向量的方法表示  点乘

![image-20250405163938111](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917807.png)

点乘

![image-20250405164045184](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917808.png)

向量化表示的好处

1、精简代码

2、提高运行速度

![image-20250405164142932](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917809.png)

![image-20250405164213604](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917811.png)

多元线性回归梯度下降

![image-20250405164311870](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917812.png)

跟一个特征的时候差不多，但是f里面的参数是向量

![image-20250405164400079](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917813.png)

特征放缩--可以快速的进行梯度下降的方法

进行特征放缩与不进行特征放缩的损失函数对比

![image-20250405164712170](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917814.png)

特征放缩的方法

1、最大值放缩--将数据放缩到0-1之间

![image-20250405164901857](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917815.png)

2、均值归一化

![image-20250405164959895](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917816.png)

3、z-score归一化

![image-20250405165039358](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917817.png)

特征缩放的选择

![image-20250405165110368](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917818.png)

检查梯度下降是否收敛

检查损失函数与迭代次数的图

![image-20250405165159562](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917819.png)

2、设定一个很小的阈值

达到阈值之后就停止运行

![image-20250405165324924](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917820.png)

学习率的选择

学习率选择不正确产生的影响

![image-20250405165528395](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917821.png)

选择

可以选择一个较小的学习率观察迭代图，选择一个较大学习率观察迭代图，最后根据图像选择一个合适的学习率。

通常选择学习率为0.001然后尝试把它放大3倍，再次进行训练

![image-20250405165553958](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917822.png)

特征工程

根据已有的特征，构造出其他的特征放到模型之中

根据已有的知识或者直觉来设置特征

可以结合多特征回归来拟合一个非线性回归模型

![image-20250405165836040](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917823.png)

多项式工程

使用特征工程以及多特征回归拟合更复杂的模型

可以使用特征放缩，使得样本分布更加均匀

![image-20250405170058480](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917824.png)

![image-20250405170143447](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917825.png)

## 逻辑回归

线性回归解决不了的问题

![image-20250405170328832](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917826.png)

逻辑回归使用的激活函数

![image-20250405170426359](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917827.png)

相当于线性回归外面套了一层sigmoid函数

![image-20250405170511235](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917828.png)

逻辑回归输出结果所表示的意义

![image-20250405170552069](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917829.png)

逻辑回归的大致过程

![image-20250405170639334](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917830.png)

决策边界

线性边界

z=0的时候的函数图像

![image-20250405170708992](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917831.png)

非线性边界

![image-20250405170744239](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917832.png)

![image-20250405170759103](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917833.png)

损失函数

为什么逻辑回归选择均方误差作为损失函数不可以

![image-20250405170913662](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917834.png)

逻辑回归选择的损失函数

y=1的情况

![image-20250405171007048](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917835.png)

y=0的情况

![image-20250405171026802](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917836.png)

![image-20250405171101007](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917837.png)

逻辑回归简化版的损失函数

![image-20250405171136268](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917838.png)

![image-20250405171201300](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917839.png)

逻辑回归的梯度下降

跟多元线性回归的差不多，但是注意f是换成sigmoid函数

![image-20250405171309578](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917840.png)

注意 逻辑回归里面也可以进行特征缩放

![image-20250405171421015](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917841.png)

## 过拟合问题

线性回归的 欠拟合、正常、过拟合

![image-20250405171549866](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917842.png)

逻辑回归的过拟合

![image-20250405171633581](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917843.png)

过拟合的解决

1.足够多的训练样本

![image-20250405171729803](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917844.png)

2.选取部分特征

![image-20250405171815771](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917845.png)

3.正则化

将部分参数的值变为很小很小的数，降低它对模型的影响

![image-20250405171904163](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917846.png)

正则化损失函数

形成新的损失函数，第一部分控制损失函数变小，第二部分，控制参数，让参数变小

蓝各大是控制参数，不能选太大-太大过欠拟合，太大会过拟合，通常选择1、10

![image-20250405172114330](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917847.png)

正则化线性回归

b通常不进行正则化

![image-20250405172501143](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917848.png)

带入原式

![image-20250405172523942](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917849.png)

w每次怎么变小的 

![image-20250405172653514](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917850.png)

正则化逻辑回归

![image-20250405172800059](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917851.png)

## 深度学习

简化的深度学习模型

![image-20250405172940682](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917852.png)

神经网络为什么兴起了

![image-20250405173016208](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917853.png)

### 单个神经元预测

相当于一个逻辑回归

![image-20250405173050904](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917854.png)

### 多个神经元

相当于使用了多个逻辑回归计算

![image-20250405173130011](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917855.png)

多隐藏层神经网络

![image-20250405173225060](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917856.png)

图像识别的例子

![image-20250405173305009](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917857.png)

![image-20250405173323893](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917858.png)

神经网络中的层

以sigmoid函数为例

![image-20250405173447598](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917859.png)

![image-20250405173519385](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917861.png)

![image-20250405173533070](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917862.png)

更复杂的神经网络

![image-20250405173651744](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917863.png)

### 前向传播

根据前面计算出的数值再次进行计算

随着网络越接近输出层，其神经元的个数越少

![image-20250405173744260](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917864.png)

![image-20250405173812861](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917865.png)

![image-20250405173831236](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917866.png)

代码中的推理

![image-20250405174008698](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917867.png)

tensorflow中的数据

![image-20250405174104638](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917868.png)

![image-20250405174122474](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917869.png)

tensor是一个张量而不是数组

![image-20250405174213946](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917870.png)

构建一个神经网络

使用的是密集层--每一个神经元将前面神经网络的输出，全部作为输入

![image-20250405174302342](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917871.png)

![image-20250405174320241](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917872.png)

![image-20250405174344706](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917873.png)

在一个单层中实现前向传播

一个一个的计算，计算的结果组成一个向量传递到下一个

![image-20250405174455259](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917874.png)

### 2.前向传播

一般实现

使用函数

![image-20250405174605102](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917875.png)

神经网络使用向量的方式高效实现

![image-20250405174703206](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917876.png)

矩阵乘法代码

![image-20250406090039479](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917877.png)

矩阵实现前向传播

![image-20250406090457047](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917878.png)

### tensorflow中训练模型

1、构建一个模型

2、指定损失函数

3、训练模型

![image-20250406091012278](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917879.png)

#### 模型训练的细节

![image-20250406091711233](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917880.png)

1、创建模型

![image-20250406092348294](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917881.png)

2、损失函数  逻辑回归的损失函数在神经网络中叫二元交叉熵损失

![image-20250406092247701](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917882.png)

3、使用反向传播的方法修改参数

![image-20250406092749261](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917883.png)

### sigmoid函数的替代

1、线性激活函数--没使用激活函数

2、sigmoid激活函数

3、relu激活函数

4、softmax激活函数--之后学

![image-20250406093916000](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917884.png)

softmax函数

可以用来解决多分类的问题--是sigmoid函数的扩展 如果特征等于2，那么它输出的结果就是一个逻辑回归

![image-20250406101623825](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917885.png)

softmax的损失函数

也是逻辑回归的推广

![image-20250406102152649](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917886.png)

### 如何选择激活函数

1、输出层选择激活函数的方法

根据输出的值的不同

二分类问题--sigmoid

需要有正负值--线性激活函数

只能有非负值--relu

![image-20250406094408264](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917887.png)

2、隐藏层选择激活函数

通常使用relu函数--1、计算比较简单，2、调参的时候更快

![image-20250406094835179](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917888.png)

激活函数总结

![image-20250406095256503](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917889.png)

### 为什么要选择激活函数

如果使用线性函数的话那么整个神经网络就是一个线性回归

如果隐藏层全部使用线性函数，输出层使用sigmoid函数，那么神经网络就是一个逻辑回归

![image-20250406100047135](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917890.png)

### softmax在神经网络中的应用

计算要使用全部得出来的值--这与其他的激活函数是不同的

![image-20250406102815938](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917891.png)

tensorflow中实现方法

损失函数使用稀疏类别交叉熵函数

![image-20250406103127570](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917892.png)

改进

softmax的改进

原因：在计算机中上层的代码会产生误差

不使用计算的中间值进行传参

改进之处：1、将最后一层的激活函数变成了线性激活函数

2、将参数传入了损失函数之中

![image-20250406104757511](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917893.png)

原因：在计算机中上层的代码会产生误差

改进之处：1、将最后一层的激活函数变成了线性激活函数

2、将参数传入了损失函数之中

![image-20250406104308012](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917894.png)

逻辑回归中的改进

1、最后一层激活函数改为线性激活函数

2、将参数传入损失函数中

3、最后实现预测的时候使用sigmoid映射

![image-20250406104929385](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917895.png)

### 多标签分类问题

![image-20250406154124146](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917896.png)

解决方式

1.分别使用三个神经网络

2、只使用一个神经网络

![image-20250406154143689](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917897.png)

## 更高级的神经网络

### 优化算法--adma算法

可以自己调整学习率--为每一个学习率单独调整，比梯度下降算法更快

1、如果w，b一直朝着相同的方向，则增大学习率

2、如果w，b来会震荡，就减少学习率

![image-20250406155309923](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917898.png)

代码中实现

![image-20250406155509694](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917899.png)

### 卷积神经网络

第一层隐藏层为卷积层--读取前面输入的部分内容--1.更快的计算，2、可以用来防止过拟合

第二层也为卷积层，最后一层是一个sigmoid层用来进行二分类

![image-20250406160626058](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917900.png)

### 有效的构建机器学习

机器学习可能的解决方法

![image-20250406161331491](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917901.png)

#### 模型评估

##### 1、划分数据集

![image-20250406161831495](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917902.png)

线性回归上的使用

2、用训练集拟合模型

3、计算模型的测试误差与训练误差

![image-20250406162202292](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917903.png)

同样的方法可以使用到逻辑回归上

![image-20250406162544676](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917904.png)

#### 模型的选择

将数据集分成两个部分，训练集，测试集

选择不同的模型进行拟合，并且将训练好的模型应用到测试集中（用来测试计算出来的参数），计算误差，选取误差最小的，但是不推荐这样，误差是乐观误差

![image-20250406163329913](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917905.png)

改进

将数据集分为，训练集、交叉验证集、测试集

下图给出每一次集的误差计算公式



![image-20250406165208927](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917906.png)

模型选择

1、使用训练集拟合参数

2、使用交叉验证集计算误差选择合适的模型

3、使用测试集来看模型的性能（计算测试集误差）

![image-20250406165949923](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917907.png)

同样的方法也可以使用到神经网络之中

选取不同的网络结构

![image-20250406170227838](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917908.png)

#### 偏差/方差

通过偏差/方差来看模型的表现

如果训练误差较大--欠拟合

如果交叉误差远大于训练误差--过拟合

如果这两个差不多--差不多

![image-20250406173509725](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917909.png)

使用一张图表示上边的例子

![image-20250406174251567](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917910.png)

蓝各大对模型的影响

1、如果蓝各大很大，参数就会很小，会出现欠拟合，造成训练误差很大，交叉验证误差很大

2、如果蓝各大很小，参数就会很大，会出现过拟合，造成训练误差较小，交叉验证误差远远大于训练误差

3、选择合适蓝各大

![image-20250406181851205](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917911.png)

使用一个图片来解释

![image-20250406182726982](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917912.png)

如何选择一个合适的蓝各大

1、尝试各种蓝各大，计算出最小的损失函数（得到相对应的参数）

2、使用得到的参数计算交叉验证的损失函数

3、通过交叉验证的损失函数，选择误差相对小的模型，如果想验证泛化误差，就是要测试集计算

![image-20250406182142189](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917913.png)

#### 测试一个模型的好坏

使用人类在这项任务上的表现来判断--这个就是基线

![image-20250407214417223](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917914.png)

通过基线和训练误差可以判断一个模型是否有高偏差的问题

通过训练误差以及交叉验证误差可以判断一个模型是否有高方差的问题

![image-20250407214959990](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917915.png)

#### 学习曲线

正常的学习曲线

横着--训练集的大小

纵轴--误差大小

![image-20250407220303934](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917917.png)

高偏差的学习曲线

数据集越大训练误差和交叉验证误差会趋平--因为是欠拟合，无论数据集怎么增大模型都不会再提升

通过基准标准与训练误差对比可以发现是高偏差

这种情况下要做的事情不仅仅有增加训练数据集

![image-20250407220811038](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917918.png)

高方差学习曲线

这个情况下增大数据集可能会有更好的效果

![image-20250407221428645](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917919.png)

#### 知道模型高偏差/高方差之后该做些什么

![image-20250408164225431](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917920.png)

#### 神经网络中的高偏差/高方差

如果有高偏差--增大神经网络

如果有高方差--收集更多的数据

![image-20250408165026797](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917921.png)

一个大的神经网络用了适当的正则化通常要比一个小的神经网络表现要好

但是有一个缺点--计算消耗大

![image-20250408165514365](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917922.png)

tensorflow中带有正则化的代码

![image-20250408165623011](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917923.png)

#### 机器学习的迭代过程

1、选择模型与数据

2、训练模型

3、根据模型训练的效果经行下一步的判断

![image-20250408171045594](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917924.png)

#### 错误分析

针对于交叉验证中出现错误的数据集进行分析 如果数据集较大，可以抽取其中的部分数据进行分析

发现出现错误**最多**的部分，根据错误信息选取合适的处理方式

比如垃圾邮件中 药物邮件的错误率较高--可以提升与药物有关的相对于的邮件的信息

![image-20250408172016780](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917925.png)

#### 添加数据

方法--添加一些扭曲、添加一些噪声、添加一些噪点

![image-20250408173444901](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917926.png)

使用人工合成数据

![image-20250408174331261](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917927.png)

#### 迁移学习

通过一个已经训练好的神经网络，使用它的参数--有监督预训练

注意使用相同的输入 比如进行图片的识别 语音的识别

有两个选项，1、输出层不同，使用自己的输出层，其他层使用已经训练好的参数，最后已经自己训练，然后进行微调

2、参数全部自己训练

![image-20250408210447149](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917928.png)

迁移学习的两个步骤

1、下载别人已经训练好的模型，或者自己已经训练好的模型

2、对自己任务结果，进行模型微调--可能只需要较少的数据就能训练出一个很好的模型

![image-20250408211614780](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917929.png)

### 机器学习的全过程

1、确认目标--图片识别、语音识别

2、收集相对于的数据

3、训练模型--误差分析、错误分析、调整参数、或者收集更多的数据

4、模型部署--监督模型，当模型的预期效果不好的时候再次进行模型的调整

![image-20250408212229056](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917930.png)

服务器部署

通过调用api的方式，对模型进行使用

可能会需要一些软件工程类的辅助以确保模型能高效的发挥作用

![image-20250408212934225](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917931.png)

### 准确率/召回率

能判断一个算法是否每次输出都是0 如果都是0准确率召回率就为0  --稀疏类别 -- 比如两种类别的数据比列不均衡

准确率--预测正确的比率

召回率-- 能帮助我们检测学习算法是否总预测 0

![image-20250509164827141](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/image-20250509164827141.png)

![image-20250409212440545](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917932.png)

权衡准确率/召回率

需要设定一个阈值--根据应用的不同

如果一个阈值很高--准确率就会很高，召回率会很低

如果一个阈值很低--准确率就会很低，召回率就会很高

需要选定一个点进行权衡

![image-20250409214628771](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917933.png)

F1分数--更关注更小的值

为了根据准确率/召回率判断一个模型的好坏，引入一个值F1分数，通过f1分数判断模型的好坏，分数高模型相对较好

![image-20250409215216364](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917934.png)

## 决策树

![image-20250410093717745](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917935.png)

### 训练过程

每一步都根据其中的数据选取选取不同的特征

如果一个数据中全是相同种类的 则不许要进行划分--不许要选取特征

![image-20250410094222705](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917936.png)

1、选取最大纯度--选取的特征能最大化的区分他们的类别

![image-20250410094539326](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917937.png)

2、何时停止划分

1、当一个节点中的数据都是同一个类别

2、树的深度不能超过树深的阈值

3、当纯度低于相对应的阈值

4、当一个类别中的例子低于相对应的阈值

![image-20250410095107352](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917938.png)

### 熵

来表示一个类别中数据的纯度

是一个关于p1的函数h

如果h越大，说明这个类别越不纯，h越小，这个类别越纯--里面的数据属于同一个类别的程度

![image-20250410095811907](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917939.png)

熵函数

![image-20250410100557209](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917940.png)

根据熵来选择特征进行分类

熵减少--信息增益

熵减少--通过计算未分类的熵与分类之后加权平均值的熵的差值来反映

选取熵减少最多的特征来进行分类

![image-20250410101920677](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917941.png)

信息增益的公式

h是熵的公式

p是一个类别中正例的比例--比如在一堆数据中判断猫，猫的个数比上总数就是p

w是分到这一类的数量占总数量的多少

![image-20250410102400859](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917942.png)

### 分裂全过程

1、所有的数据都在根节点

2、选取特征，计算信息增益，根据信息增益选取一个特征进行分裂

3、分裂数据，将数据分发到左右子树

4、根据以上步骤再次进行分类直到到达相对应的阈值就停止

![image-20250410103153465](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917943.png)

### 独热编码

将一个特征（取值情况大于2种的离散取值）分为多个特征，每个特征取值结果为0/1

![image-20250410164726018](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917944.png)

### 决策树处理连续的值

取一个值，计算信息增益，选取一个信息增益最大的阈值进行数据集的划分

![image-20250410165436063](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917945.png)

### 回归树

通过已有的特征去预测一个连续的值，例如本例，根据特征去预测体重，体重是根据训练数据的平均值求的

![image-20250410170611453](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917946.png)

回归树的分裂

类似于信息增益，每次根据一个特征进行分类，计算分类后/前的方差，求分类后加权平均值方差，最后用未分类之前的方差减去分类之后的方差，选取一个方差减少最大的。

![image-20250410170814902](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917947.png)

### 使用多个决策树

使用多个决策树的原因--如果只是用一个树，其中源数据只变化一个例子，那么这个树选择的分裂特征就是不同的，那么就构建了不同的树，这说明一个树对于源数据的变化比较敏感

![image-20250410171508118](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917948.png)

多个树

解决的方法就是构建多个树，然后对于要预测的例子分别放到这三个树上预测，最后进行投票，根据投票的结果决定预测的结果

比如本例，预测结果为2个猫一个狗，则最后的结果就是猫

![image-20250410171645963](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917949.png)

#### 放回抽样

把数据放到一个看不见的箱子里面，随机抽取数据，抽取完成之后，在放回箱子之中，再次进行抽取

目的是构建不同的数据集

![image-20250410172320320](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917950.png)

#### 袋装决策树

使用放回抽样的方式实现的

如果有m个数据，放回抽取m次，根据数据构建一个决策树，然后将这个步骤重复B次B通常在64-128之间

最后构建B颗树，通过投票决定最终预测的结果

![image-20250410173132286](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917951.png)

#### 随机森林

袋装决策树的升级版--随机抽取k个特征，根据特征建设不同的树，k的取值为根号n，n为数据本身的特征的数量

随机森林比单个决策树更加稳健

![image-20250410173909607](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917952.png)

### 提升树

对于决策树的训练，先根据放回抽样的方式以相同的概率抽取数据集，根据抽取的数据集进行模型的训练，然后使用原数据集进行测试，对于预测错误的数据，在下一次抽取时提升它被抽取到的概率，一直迭代以上的方式，知道B次

![image-20250410175405218](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917953.png)

XGBoost

跟提升树一样，以下是它比较好的地方

1、应用于提升树并且是开源

2、有很高的效率

3、有默认的选择分类的特征以及合适停止

4、有正则用来防止过拟合

![image-20250410175717075](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917954.png)

代码的实现方式

![image-20250410175954008](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917955.png)

决策树与神经网络的对比

1、决策树更适合结构化的数据，不适合非结构化的数据，训练的比较快，小的决策树更容易被解释

2、神经网络不仅适合结构化的数据也适合非结构化的数据，训练时间较长，但是串联和训练多个神经网络比决策树容易

![image-20250410180745782](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917956.png)

## 无监督学习

### k-means

步骤：1、随机选取分类的点，比如本类2个点

2、扫描所有的数据，查看数据离着那个簇心较近就把它分配给那个簇质心

3、分配完之后，计算相同簇的质心，并将簇质心的叉移动到本簇质心的位置

4、重复2、3步骤，一直到质心的位置不改变的时候那么这个算法就收敛了，计算完成

![image-20250411093147351](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917957.png)

具体算法

如果一个中心经过一次扫描数据计算没有给他分配一个函数，通常的做法是直接删除这个聚类点

![image-20250411095404400](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917958.png)

#### 损失函数

每一个点到聚类中心的距离的平方的均值  最小化这个损失函数

![image-20250411100934086](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917959.png)

#### 随机初始化的方法

随机初始化聚类中心，然后进行聚类算法，得到损失函数

将这个步骤重复多次，最后选取一个损失函数较小的的一个聚类

![image-20250411102131838](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917960.png)

#### 如何选择k

k是聚类中心的个数

1、根据elbow算法，通过观察k的变化与损失函数的变化，有一个变化较大的点，那么就选择那个点作为k

![image-20250411103440979](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917961.png)

2、根据这个聚类算法的业务来进行选择，先训练出一个聚类，然后根据这个聚类算法的表现情况来选择k的取值

![image-20250411103604442](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917962.png)

### 异常检测

#### 基础知识

将训练集进行建模，给定一个阈值，如果训练一个值，发现小于阈值则说明它是有问题的

![image-20250411110136277](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917963.png)

使用高斯分布（正态分布）进行建模

![image-20250412092801913](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917964.png)

不同的均值和标准差将会构建不同的图

![image-20250412092849163](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917965.png)

对于单个变量的使用

![image-20250412092929509](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917966.png)

高斯分布用于多个特征

对于每一个特征按照高斯分布构建模型，然后再相乘

![image-20250412094416126](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917967.png)

#### 建设过程

1、选取数据的特征

2、计算数据特征的平均值和方差，进行高斯建模--可以使用向量化的方式，提升计算速度

3、对于检测数据，带入他的特征到所有的高斯模型中，然后将结果乘起来

4、根据阈值进行判断

![image-20250412095031632](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917968.png)

以二维特征举例

计算每一个特征的平均值、方差然后进行高斯建模，设定一个阈值0.02

根据测试数据，计算数据每一个特征的概率，然后将他们相乘，相乘的概率类似于左下的图像，最后计算的结果与阈值进行对比。

![image-20250412095623279](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917969.png)

#### 如何验证模型的好坏

1、可以使用有监督的方式来验证，有10000个正常的数据以及20个不正常的数据

将数据集进行划分，训练集、cv集、测试集，训练集是没有标签的，因为异常检测是无监督学习

建设完模型之后，使用cv集进行阈值的选择，根据选择的阈值，查看模型检测出多少正确的数据以及多少错误的数据，根据检测数据情况对于阈值进行更改

选定好阈值之后，可以使用测试集来测试一下模型在实际应用的时候表现情况

2、如果异常数据比较少，可以不分配测试集数据。

![image-20250412100834198](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917970.png)

异常检测和监督学习的选择

1、当只有一部分正面数据和大部分反面数据的时候，适合使用异常检测，当有大量的正面数据以及反面数据的时候，使用监督学习比较方便

2、当出现的异常是原来数据中没有的适合使用异常检测，出现的异常时数据中有的适合监督学习

以下是例子

![image-20250412103733795](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917971.png)

#### 选取特征

先看特征的直方图，查看是否与正态分布相似，若相似可以直接使用，如果不相似，可以转换成类似高斯分布的图再进行使用

如果特征进行了转换，注意该特征的交叉验证集以及测试集也要跟着转换

![image-20250413094956585](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917972.png)

错误分析

假如只有一个特征，x1，但是出现异常的p与正常的p非常相似，那么可以继续研究这个特征，发现为什么它是异常的数据，可以根据这个特征去创造一些新的特征，确保正常的p比较大，而异常的p比较小

![image-20250413100229710](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917974.png)

例子

![image-20250413100425983](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917975.png)

### 推荐系统

#### 一、协同过滤

#### 1、根据电影特征预测用户评分

获取每个电影的特征，比如浪漫评分、动作评分

根据每个电影的特征，拟合每一个用户的线性回归模型，然后进行预测

![image-20250413102810322](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917976.png)

损失函数

注意符号：r（i，j）表示用户j是否对i电影评分

y（i，j）用户j对i电影的评分是多少

w(j),b(j) j用户拟合线性回归的参数

x(i) 电影的特征

m（j）j用户评论电影的数量

损失函数类似于线性回归的损失函数，注意求和的数量是已经评分的电影的数量

![image-20250413102955204](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917977.png)

第一个，在推荐系统中可以直接删除m（j） 这样求损失函数比较方便，并且最后得到的效果是一样的

第二个 也可以一下学习所有用户拟合线性回归的参数

![image-20250413103319387](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917978.png)

#### 2、根据用户评分拟合电影特征

通过给定的用户评分以及参数用来猜测电影的特征

![image-20250413105112633](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917979.png)

电影特征的损失函数

![image-20250413105139538](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917980.png)

将1、2两个损失函数放到一起--协调过滤的损失函数 通过一个损失函数来修正w b x

![image-20250413105847507](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917981.png)

梯度下降

![image-20250413110247802](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917982.png)

#### 3、泛化到二元标签

二元标签数据

![image-20250413113257535](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917983.png)

损失函数

![image-20250413113326892](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917984.png)

#### 4、均值归一化

问题：如果一个新用户未对电影进行评分，那么会认为它将给所有的电影评分为0

![image-20250414094349668](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917985.png)

解决：使用行均一化处理，会对使得新用户对于电影的评分为其他大众评分的均值，更加合理

并且使用均值归一化也可以使得算法表现的更好

![image-20250414094505869](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917986.png)

#### 5、在tensorflow中的实现

tensorflow中有自动diff 可以自己求导数

![image-20250414095900595](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917987.png)

协同过滤

其中加入了adam算法--自动调整学习率

![image-20250414100008416](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917988.png)

#### 6、推荐相关的商品

每一个商品都有一个特征，计算特征之间的距离，选取最小的进行推荐

![image-20250414101219464](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917989.png)

协同过滤的局限性

冷启动问题--对于一个新用户来说推荐商品，对于一个新商品没有评分应该如何推荐

没使用到别的信息--用户的性别、年龄，电影的明星

![image-20250414101314011](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917990.png)

### 二、基于内容过滤

考虑用户的特征以及相关商品的特征

![image-20250414102818120](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917991.png)

根据特征去构建一个特征向量，注意这个特征向量是不一样大小的

![image-20250414102857167](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917992.png)

构建模型

不在使用w、x使用v，v是从用户/商品特征中计算而来的，注意v的大小是一样的，这样可以进行点积运算

![image-20250414102939264](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917993.png)

#### 用神经网络实现

从x中计算v ，x是事物的特征，v是提取重要的内容

![image-20250414104115310](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917994.png)

损失函数

前提，你有一些用户对于电影的评分

注意没有单独训练vu和vm的损失函数，要把他俩个连在一起，同时进行调参

![image-20250414104418520](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917995.png)

应用

可以计算出某个电影的特征，然后与用户喜欢的电影的特征进行计算，计算距离小的进行推荐

![image-20250414104706015](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917996.png)

#### 对于含有大量数据集的应用解决的方法

分为两个步骤

1、检索--对于用户最近观看的10部电影，挑选10个与之相关的电影，计算方式是使用特征的方式

选取电影其中的三个类别，选则三个类别中排名前10的电影

选取这个用户所在国家的观看的前20个电影

把所有选取的电影放在一起，去除其中重复的、或者用户已经看过的

![image-20250414110006471](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917997.png)

2、排序--对于检索到的电影，放到模型中计算预测评分，然后进行排序，展示排序给用户

![image-20250414110337529](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917998.png)

检索步骤的建议

通常选取的电影越多，效果越好但是缺点就是计算的时间比较长

通过线下实验，查看通过增加检索量，对于提升相关性有多大的作用

![image-20250414110529699](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917999.png)

tensorflow实现

注意vu/vm最后实现一下L2标准化--这样可以让模型表现的更好

![image-20250414170955260](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917000.png)

## 强化学习

举例：如果有一个无人机，如何让他自己学会控制自己飞行，可以有奖励函数，如果做的好，奖励就加，如果做的不好，奖励就减少

![image-20250414174555735](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917001.png)

以火星探测车为例

火星探测器会有一个状态，比图状态4，一共有6个状态，每个状态对应着不同的奖励，采取不同的行动会得到不同的奖励

1、6都是它的最终状态

（s,a,R(s),s1) s是火星探测车的状态，a是它的动作，R（s）是s状态下的奖励，S1是动作之后的状态

![image-20250414174752559](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917002.png)

### 回报

每采取一个行动都会有一个奖励，并且每一步的行动都会有一个折现因子（通常是一个略小于1的数）

第一步获取的奖励最大，之后的每一步奖励都减少，根据最后的状态计算奖励

![image-20250414175049500](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917003.png)

可以根据探测车不同的状态计算奖励（计算方法同上），比如探测车在状态4，一直朝左移动的奖励，一直向右移动的奖励，计算出来之后可以根据奖励方便操作探测车在每个状态下应该去什么方向走获得奖励最大

![image-20250414175239642](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917004.png)

### 强化学习的策略

通过不同的策略（比如向着回报较大的方向走）建立不同的pi函数，可以通过pi函数，指明现在状态下朝那边走可以获得最大回报的策略

![image-20250414175839991](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917005.png)

### 强化学习的应用

![image-20250415211728113](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917006.png)

马尔可夫决策过程

未来的状态只取决于现在

![image-20250415211951747](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917007.png)

### 状态行动值函数（Q函数）

Q（s，a）s是现在的状态，a是采取的行动，Q会返回这次行动之后最优的策略（比如计算中蓝色的）

Q（2，右）最好的策略是从2出发向右走、向左走、向左走，计算方式是当前状态+下一步折现因子*回报+。。。

![image-20250415213151203](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917008.png)

目的是为了选择一个较好的pi函数

根据计算的Q（s，a），pi（s）选择的就是当前状态下Q的最大值

![image-20250415213638805](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917009.png)

### 贝尔曼方程

用来计算Q函数

s是当前状态，a是这一次的动作，s1是下一次的状态，a1是下一次的动作

根据贝尔曼方程可以计算Q

贝尔曼方程等于当前的奖励+折现因子*下一次动作的最大奖励（下一次动作的最优过程）

![image-20250415220010561](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917010.png)

贝尔曼方程等于当前的奖励+折现因子*下一次动作的最大奖励（下一次动作的最优过程）

![image-20250415220124481](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917011.png)

例子

![image-20250415220259160](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917012.png)

### 随机环境

以火星车为例，当火星车在状态4的时候，如果下一个动作向左走，则它会有0.9的概率向左走，会出现0.1的概率向右走（车轮打滑、环境不好）如果下一个动作向右走，则它会右0.9的概率向右走，会出现0.1的概率向左走

![image-20250416094656336](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917013.png)

#### 期望回归

在随机环境的条件下，火星车在一个状态下执行多次，去多次的平均值作为这个状态的期望回归

![image-20250416094926494](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917014.png)

修改一下贝尔曼方程，等于即使奖励+折现因子*期望回归

![image-20250416095049608](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917015.png)

### 连续状态

取值不在是单一的离散值，而是一个向量，其中包括多个状态

![image-20250416100047028](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917016.png)

#### 深度学习训练Q函数

输入的x是一个向量，包括当前的状态（x，y，***）和动作（向左开启推动器、向右....）

使用监督学习的策略来训练深度网络

![image-20250416102206570](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917017.png)

获得带有x，y的训练集

通过贝尔曼方程构造x，y函数，会有4个状态（s，a，R（s），S1），前两个是当前状态以及动作，后两个是经过动作之后的回报，以及动作之后的状态，根据作为x，后面两个计算得到y，（Q是猜出来的，刚开始随机，随着网络的运行不断变好）

![image-20250416102412477](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917018.png)

总结---DQN算法/Q算法

1、随机生成神经网络的参数，包括Q

2、以月球登陆为例，随机采取行动获得一个元组（s，a，R（s），s1）

3、记录最近的10000个元组信息，

4、训练神经网络，x=元组的前两个，y是根据前两个算出来的贝尔曼方程的结果，训练Q（x）一直到可以很好的得出y，然后将所有的Q变为训练好的Qnew

5、不断重复以上过程

![image-20250416103245143](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917019.png)

#### 改进1

上面的神经网络只有一个输出，如果想得到动作的最大值，需要运行4次，改进的方法是输出变为4个，只需要运行一次神经网络便可以得出

![image-20250416104253173](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917020.png)

#### 改进2

每次的选择不在是选择最大的Q，而是以0.95的概率选择最大的，0.05的概率随机选择一个--epsilon-贪心算法

通常设定epsilon为一个很大数，随着模型的训练逐渐减小

设定epsilon的原因是，有时候初始化参数的时候，可能某个动作参数设定的不好，但是其实在一个状态下可能采取那个动作得到的效果更好，所以采取0.05概率测试除回报高的其他动作

![image-20250416105939079](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917021.png)

#### 改进3

小批量处理

小批量处理，如果有一个很大数据集，每次去一小部分作为训练集，用这选中的数据集来训练模型，并且进行梯度下降，

然后再选择一部分数据，拟合进行梯度下降

![image-20250416112049675](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917022.png)

小批量处理的梯度下降图

![image-20250416112536892](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917023.png)

应用到强化学习

保存10000个数据集，选取其中1000个进行训练，但是会出现的情况是Q可能会变差，因为每一次训练都是选取1000个并且每次训练结束都会更新Q，所以为了防止Q变得更坏采取--软更新策略

![image-20250416113043364](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917024.png)

软更新

设定两个参数（两个权重），表示训练完模型之后旧参数向新参数变化的趋势，可以设定Q向Qnew变化的趋势，

![image-20250416113236230](https://cdn.jsdelivr.net/gh/onlywater1/blog-picture/202505221917025.png)